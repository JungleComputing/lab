Final report includes

title
-----

Using the Maestro data-flow middleware for the Data Analysis Challenge

Kees van Reeuwijk
Vrije Universiteit Amsterdam
reeuwijk@few.vu.nl



Team information
----------------

Team dach001 consists of one member: Kees van Reeuwijk.  However,
the entry heavily uses the Ibis middleware, so the support from the
various members of the Ibis team was invaluable for this program.
In particular Jason Maassen and Ceriel Jacobs were very helpful.

It is also worth pointing out that there is another team from the
VU, dach004, which in fact has Jason Maassen as team leader. The
software described in this report is an independent effort, and no
competition software from dach004 was used for this effort, or vice
versa.  However, experiences and performance results were freely
shared between the two teams, and were very helpful for the development
of this program.




The used software
-----------------

The purpose of participating in this challenge was to test the usefulness
of Maestro, a middleware for self-organizing data-flow computations, on
a system of the scale of Intrigger, and for a real-life application
such as the astronomy application behind the Data Analysis Challenge.

Maestro is built on top of Ibis <www.cs.vu.nl/ibis>, an
open source Java grid software project of the Computer Systems
group, which is part of the Computer Science department of the
Faculty of Sciences at the Vrije Universiteit, Amsterdam, The
Netherlands. The main goal of the Ibis project is to create an
efficient Java-based platform for grid computing.

Maestro is a newly developed component of Ibis for data-flow
computations.  One of the important features of Maestro is that as
much as possible it is self-organizing. In particular, distribution
of the computations over the nodes in the system is based on local
decisions by each of the nodes, not by a central coordinating
process.  The only special nodes in the systems are the `Maestro'
nodes that put input data into the system and handle the final
result, but apart from this additional functionality, they behave
like all other nodes in the system. Each node has a work queue that
contains tasks to be executed. If a completed task is not the last
step in a computation, it generates a new task that is then submitted
to the best node in the system for execution, either the local
node, or another one in the system. To be able to select the best
node, the capabilities and performance of all the nodes in the
system are continuously `gossiped' by a background process on each 
node. The number of tasks in the work queues of the nodes is kept to
a minimum to avoid idle time in the processors, while still allowing
rapid reaction to changing circumstances.

The Data Analysis Challenge was a welcome opportunity to demonstrate
the effectiveness of Maestro, but it was clear from the start that
the challenge would be difficult, since Maestro is still very much
in development.  Moreover, it was not immediately obvious that the
computational model of Maestro would be suitable the Data Analysis
Challenge computation.  Maestro is designed for data-flow systems,
where computations `flow' through a number of distinct processing
steps.  Fortunately, the Data Analysis Challenge computation can
be seen as the case where the number of processing steps is one,
reducing the system to a simple master/worker configuration.

Also, before this challenge, a central assumption in Maestro was
that a computation consists of a number of clearly distinct types
of task, where each task is executed many times and has a more-or-less
repeatable execution time. The nodes use these properties to learn
the best allocation of computations to processors based on previous
performance.  The computation of the Data Analysis Challenge does
not match these assumptions, since the comparison operations for
the different images have greatly varying computation times.
Moreover, since worst case only one or two computations fit in the
allocated time frame, the system cannot afford to learn from earlier
results, but must assign at least all the longest jobs correctly
in all cases.

We solved these problems by introducing a new category of tasks in
Maestro: unpredictable tasks. For such a task Maestro runs a
benchmark (an image convolution kernal for the Data Analysis Challenge
computation) on each node of the system on startup. Maestro then
preferentially submits tasks to nodes with fast benchmark results.
For predictable jobs, Maestro uses the execution time of each
completed task to update its estimate of the performance of each
node. For unpredictable tasks it ignores these execution times.
Also, it does not fill the work queue of a node with additional
tasks, as it does for tasks with predictable execution times, but
only assigns new work to a node if it has an idle processor.  Although
this increases the idle time of the processors, it avoids the
situation where one processor is idle when another processor has
tasks waiting for execution.


Since the comparison operations of the Data Analysis Challenge
have such widely varying computation times, it is important to
identify the longest computations, and start these as soon as
possible.  The required execution time of a given pair of images is
not known beforehand.  However, experiments from the dach004 team
showed that there is a reasonable correlation between image file
size and the duration of the computation, so our program uses a
simple heuristic where we assume that the pair of images with the
largest total file size will take the longest to compare.  Thus,
the master first assigns the image pair with the largest total file
size to the best processor, then the second largest to the best
still available processor, and so on.

Considering the total number of cores in the system, we could use
all the cores in the system and compare only one or perhaps two
pairs on each of them. However, since managing such a large cluster
of systems is hard, and since the communication network can be
saturated by a large load, we only used about a 130 nodes with
roughly 700 cores in competition runs, with two nodes only used for
management. Since many of the comparisons only take a few seconds,
the total execution time was still determined by the longest-running
comparison.

Another important aspect of the challenge was to get the right data
at the right place. Although initially we were planning to use
a distribution of the computation based on the location of the 
input files, we were not able to implement this in time. Instead
we simply rely on the Gfarm system provided by the Intrigger system
to handle this aspect.

Similarly, a reliable system for managing all nodes in the system
is essential. Such a system is required for installing software,
checking the configuration, cleaning up, and starting the nodes for
the computation. We chose not to concentrate on this aspect, but
rely on the 'gxpc' tool that was provided.




Results
-------

The Intrigger platform proved to be more problematic for our software
than the local clusters we normally use, due to the larger scale
of the system, the large variety of communication times, and the
instability of the Intrigger system outside our pre-assigned
time-slots. Therefore, we were forced to do extensive rewriting of
the Maestro software immediately before and even during the
competition.  Unfortunately, this meant that in all three of the
pre-assigned time-slots we were not able to complete a run,
mainly due to
internal problems in the Maestro implementation.

Only in the final extra, and extra-long timeslot we were able to
run the problem dataset entirely.  In that timeslot we were able
to do two runs that computed all pairs. Unfortunately both failed:
the program collected all results in a result file, but the dach_api
returned a FAIL verdict. It is not clear if this was only due to the
rounding errors announced by the challenge committee, or due to
some corruption of the results or image files. 

The first run produced the following results:

FAIL TRIAL-final_dach-267294 8421.87942505 hongo102 Thu Aug 14 23:30:45 2008 7b156778424f531a2180574cc1de5a67

Due to stability problems of Maestro, 21 of the 129 nodes used in the
computation failed, and some of their computations had to be restarted.
Some of these failures occurred very late in the run, and caused a very
long delay. However, the fact that we could complete the entire
run was a milestone in its own right.

In the second run we used an new version of Maestro with a number
of bug fixes.  We chose to run the FT variant since we knew through
hard-won experience that the system was able to handle crashing
nodes, so we were prepared to take our chances with the process
killer of the FT variant.

Remarkably, none of the nodes in the computation was in fact killed,
so for the first time ever Maestro ended the run with the same
number of nodes it started with: 129.  It is not clear why none of
the nodes was killed, but we note that since we used less than 50%
of all available nodes, killing all processes on a random node in
the system has a high probability of missing our computation entirely.

This time, the system was much more stable, and was able to handle all
jobs very rapidly. 

FAIL TRIAL-final_dach_ft-207557 2940.93734503 hongo102 Fri Aug 15 00:27:35 2008 dba8b531197a7e2b8ea1f64d106a4646

Thus, were the first run took more than two hours, the second one
only took about 50 minutes.

We should note that the Gfarm system might have helped us in the
second run. Assuming that the FT challenge uses the same data files
as the non-FT variant (we're not sure this is the case, although
it seems likely), the first run essentially preloaded the required
data into the Gfarm caches. Although the computations in the second
run were not always distributed in the same way as in the first,
this may have had a significant impact.

As a side note: if there are 20 pairs or less still outstanding, the
program prints all missing pairs. The first such output from this
run neatly demonstrates the validity of the largest-files-first
heuristic:

Still missing: [label#2, label#3, label#4, label#5, label#6, label#7, label#10, label#12, label#13, label#14, label#15, label#16, label#17, label#22, label#23, label#25, label#26, label#27, label#29]

Since these labels are handed out in sequential order, this shows
that all 20 pairs that were the last to complete were in the first 30
to be started.



Evaluation
----------

First, the evaluation of the Data Challenge program: Considering
that the total run took 2940 s, and the largest job took 2832 s,
it is clear that without improvements to the comparison process
only limited performance gains can be expected.  However, this
comparison process consists of two steps: accessing the input files
(we copy them to the /tmp directory to make sure they are local to
the node), and doing the actual comparison.  Unfortunately, we did
not have time to instrument the comparison process, so it is not
clear how much time is spent in fetching the input files, and how
much time is spent doing the actual comparison.

However, since only a few comparisons require anything near this
long to complete, we are clearly not using the processors
very efficiently: a lot of processors are only idling until
these computations are finally completed.

Second, the evaluation of the Maestro framework: The Data Analysis
Challenge has exposed Maestro to a more `real-world' environment
than it was ready for. This resulted in a large number of problems
in the program, and forced us to do substantial rewriting on large
parts of the implementation during the competition. The resulting
program is clearly far more mature, and whereas the original program
could only handle simple linear sequences of processing steps with
repeatable execution time, Maestro now also supports

- processing steps with unpredictable execution time

- map/reduce steps where a processing step spawns a number
of sub-computations that are then reduced into one result.
(We expected the final challenge to consist of multiple problems,
and each problem would be implemented as a map/reduce step.)

- Alternative data flows, where the system is given a choice
between different alternative ways to execute the computation,
and can choose the fastest one. (This was implemented for
a locality-aware implementation would try to do each computation
on a 'home' node of an image pair.)

- An mechanism to explicitly fail a node for a particular type of job.
This was introduced to handle comparisons that sometimes failed. By
declaring a computation a failure, it can be executed again. By
failing the node where the problem occurred, we make sure it doesn't get
the same job again.

All in all the competition has caused significant improvements to Maestro.


Future work
-----------

The current program uses the Intrigger system very inefficiently,
since for a large part of the time only a few processors are active:
they are working on the largest comparison jobs, while the other
processors have already completed all the faster jobs. Larger batches
of images would alleviate this problem somewhat, since it would
keep more processors busy for a longer time, but this it is not
clear if that is realistic.

A more interesting approach has been tried to great success by the
dach004 team: parallelizing (and thereby speeding up) the comparison
process itself. This not only decreases the execution time of the
largest job dramatically, but from as far as I understand the
parallel implementation consists of a sequences of tasks that all
have predictable execution times. That would make it a computation
that fits right in the Maestro computational model!

Another issue is data locality. We originally planned not to use
the Gfarm system, but use a 'owner computes' scheme where a comparison
is only executed on a node that has the image pair locally available.
This could be implemented in Maestro by introduction locality-aware
comparisons. Instead of only one type of task `compare-images',
there would by separate tasks for each cluster: `compare-images-on-hongo',
`compare-images-on-kobe', etc. For each image pair the system could
then choose to execute the comparison by selecting one of these
cluster-specific comparisons, depending on the clusters where the
image pair would be locally available. The optimal selection of
comparisons is not trivially determined, however. Moreover, even
restricting the computation to `owner-computes' is a restriction
that can significantly impact performance, depending on the images
over the clusters.


Conclusion
----------

Despite very serious problems in the Maestro implementation, we
were eventually able to successfully run the entire challenge. The
fact that the result file still fails is disappointing, but since
it is clear that the parallel execution of the comparisons is now
functioning, we expect that this is a minor problem, or may even be due
to rounding errors only.

Rumor has it that at the end of the final official round of the
challenge, none of the teams had been able to complete the challenge.
We know that at least team dach004 has completed the challenge in
`extra time', but in the circumstances we are quite happy with the
results.

Also, if the rumor is correct we might well be the only one mad/brave
enough to try the FT challenge. If so, and if the entry is accepted
at all, dach001 might be in both first and last place in the FT
challenge!


